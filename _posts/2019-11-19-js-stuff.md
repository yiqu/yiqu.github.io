---
layout: post
title: JavaScript Gotchas
readTime: 5
tags:
  - javascript
  - coding
  - interview
---

Javascript is usually considered a wild wild west language. There are countless memes made about how Javascript is the weird and quirky 
language that no one wants to touch. You can see many blog posts about it [here](https://blog.mgechev.com/2013/02/22/javascript-the-weird-parts/) and [here](https://charlieharvey.org.uk/page/javascript_the_weird_parts).

In this post, I would like to explore some of the common gotchas of the Javascript language.
<!--more-->

<br/>

#### Math.min() > Math.max()

<code>Math.min()</code> is greater than <code>Math.max()</code>. This sounds weird, right? If no arguments are given, <code>Math.min()</code> returns <code>infinity</code> and <code>Math.max()</code> returns <code>-infinity</code>. Why does Math.min() return Infinity?
Well <code>Math.min(arg1, arg2,..)</code> will return the smallest number in the list. 

If there is only one argument provided, then it will return that number. So if the default argument is <code>-Inifinity</code>, then for every number provided in the argument list, it will *always* return <code>-Infinity</code>, which renders the function useless. Therefore <code>Inifinity</code> is the default value, and thus if any one single argument is provided, then that number would be the result returned.

```javascript
Math.min(1) 
// 1
Math.min(1, infinity)
// 1
Math.min(1, -infinity)
// -infinity
```

<br/>

#### 0.1 + 0.2 !== 0.3 

Why does 0.1 + 0.2 !== 0.3 in Javascript? If you brought up your console and typed in that equation, you would see the following:

```javascript
0.1 + 0.2
// 0.30000000000000004
```

This is actually not a Javascript specific issue, instead a general issue with computing. 

Floating number can't store properly all decimal numbers because they store stuff in binary. Internally, computers use a format (*binary floating-point*) that **cannot** accurately represent a number like 0.1, 0.2 or 0.3 at all.

When the code is compiled or interpreted, your <code>0.1</code> is already rounded to the nearest number in that format, which results in a small rounding error even before the calculation happens.

So how to avoid this issuse? Instead of doing operations with the floating numbers, convert them to integers first, do the operation second, and display the result with decimal last.


<br/>

#### Difference of a Function Expression from a Function Declaration

In usage, the key difference is that function declarations are hoisted, while function expressions are not. That means function declarations are moved to the top of their scope by the JavaScript interpreter, and so **you can define a function declaration and call it anywhere in your code**. By contrast, you can only call a function expression in linear sequence: **you have to define it before you call it**.

```javascript
// Function Declaration. Auto hoisted, can call anywhere
function sum(x, y) {
  return x + y;
};

// Function Expression: ES5. You cannot call this anywhere*
var sum = function(x, y) {
  return x + y;
};
```

There are a lot more weird and quirky behaviors in Javascript. Look out my next post in the Javascript Gotchas series!
